{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning with Scikit Learn\n",
    "\n",
    "*Adapted from https://github.com/justmarkham*\n",
    "\n",
    "### Libraries\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/)\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "In this tutorial we will see some basic examples of Linear Regression for regression and Logistic Regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Linear Regression\n",
    "\n",
    "|     *            | continuous     | categorical    |\n",
    "| ---------------- | -------------- | -------------- |\n",
    "| **supervised**   | **regression** | classification |\n",
    "| **unsupervised** | dim. reduction | clustering     |\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Why are we learning Linear Regression?\n",
    "- widely used\n",
    "- runs fast\n",
    "- easy to use (not a lot of tuning required)\n",
    "- (in some cases) highly interpretable\n",
    "- basis for many other methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "- Sales: sales of a single product in a given market (in thousands of units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "data.plot(kind='scatter', x='TV', y='sales', ax=axs[0], figsize=(16, 8), grid=True)\n",
    "data.plot(kind='scatter', x='radio', y='sales', ax=axs[1], grid=True)\n",
    "data.plot(kind='scatter', x='newspaper', y='sales', ax=axs[2], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on!\n",
    "Let's create the feature matrix and the class vector (X and y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'radio', 'newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn** provides an easy way to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()  # create the model\n",
    "lin_reg.fit(X, y)  # train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the theory! Let's see what the formula looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(feature_cols)):\n",
    "    print(\"{0} * {1} + \".format(lin_reg.coef_[f], feature_cols[f]))\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1  \\times TV + \\beta_2  \\times radio + \\beta_3  \\times newspaper$$\n",
    "$$y = 2.938 + 0.045 \\times TV + 0.18  \\times radio + -0.001  \\times newspaper$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predictions and the original values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have only 3 features, but 200 records, which is enough for learning a good linear model. But what if we had much fewer records, say, 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:5]\n",
    "y = y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "predicted = cross_val_predict(lr, X, y, cv=5)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LinearRegression()\n",
    "logistic.fit(X, y)\n",
    "predicted_train = logistic.predict(X)\n",
    "mean_squared_error(y, predicted_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: The model remembers the training records (overfitting).\n",
    "\n",
    "**Solution**: Regularization\n",
    "\n",
    "Regularization refers to methods that help to reduce overfitting. Let's try Ridge Regression, which puts a penalty on large weights $\\beta_i$ and forces them to be smaller in magnitude. This reduces the complexity of the model. More on this in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=6)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(y, predicted_r, edgecolors=(0, 0, 0))\n",
    "ax.plot([min(y), max(y)], [min(y), max(y)], 'r--', lw=4)\n",
    "ax.set_xlabel('Original')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, predicted_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression\n",
    "\n",
    "|*|continuous|categorical|\n",
    "|---|---|---|\n",
    "|**supervised**|regression|**classification**|\n",
    "|**unsupervised**|dim. reduction|clustering|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_raw = pd.read_excel('data/titanic.xls')\n",
    "titanic = titanic_raw.dropna(axis=0, how='any')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- sex\n",
    "- age\n",
    "- fare: ticket price\n",
    "\n",
    "What is the **response**?\n",
    "- survived: whether the passenger survived the disaster or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = titanic[titanic['survived']==0]\n",
    "survived = titanic[titanic['survived']==1]\n",
    "\n",
    "print(\"Survived {0}, Dead {1}\".format(len(dead), len(survived)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the columns to use as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features = ['sex', 'age', 'fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the feature vector for the training\n",
    "\n",
    "The dataset contains one categorical variable: sex (male|female)\n",
    "\n",
    "We need to convert it to a numerical variable. Pandas offers the method *get_dummies* that takes care of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features vector\n",
    "X = pd.get_dummies(titanic[titanic_features])\n",
    "X.head()\n",
    "# titanic['pclass'] = titanic['pclass'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label used for the traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# for f in range(len(feature_cols)):\n",
    "#     print(\"{0} * {1} + \".format(logistic.coef_[f], feature_cols[f]))\n",
    "print(logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and evaluate the precison/recall with a cross validation (10 splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "# Precision: avoid false positives\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "# Recall: avoid false negatives\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the model output\n",
    "\n",
    "Let's train on the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, since we trained the whole dataset, we don't have new samples to predict, but we can predict the outcome and the relative probability for some artificial samples. Would you have survived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict([[25, 100, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict_proba([[25, 100, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict([[25, 100, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic.predict_proba([[25, 100, 1, 0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
